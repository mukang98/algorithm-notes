{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 线性回归实现流程\n",
    "1. 生成数据，构造输入输出数据对，并构造数据迭代器  \n",
    "2. 参数初始化  \n",
    "3. 定义模型结构  \n",
    "4. 定义损失函数  \n",
    "5. 定义优化方法  \n",
    "6. 初始化超参：学习率、epoch  \n",
    "7. 模型训练（注入数据，正向传播，计算loss，反向传播，计算梯度，更新参数）  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 实现流程差异之基于框架与非基于框架"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 区别一：构造迭代器\n",
    "- **非基于框架**：\n",
    "手动构造数据迭代器，需要对数据进行索引、打乱并按批次返回数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import torch\n",
    "\n",
    "def data_iter(batch_size, features, labels): \n",
    "    num_examples = len(features)  # 获取训练数据的长度\n",
    "    # indices = list(range(num_examples))  # 根据长度生成索引\n",
    "    # random.shuffle(indices)  # 打乱索引\n",
    "    indices = torch.randperm(num_examples)  # 使用 torch.randperm 生成随机索引,使用 torch.randperm() 替代 random.shuffle()，可以避免原地修改列表索引，在多线程环境中更安全。\n",
    "    for i in range(0, num_examples, batch_size): \n",
    "        batch_indices = torch.tensor(indices[i: min(i + batch_size, num_examples)])  # 获取每个批次的数据索引\n",
    "        yield features[batch_indices], labels[batch_indices]  # 使用 yield 生成迭代器\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **基于框架**：\n",
    "通过 TensorDataset 和 DataLoader 快速构造迭代器，提供数据打乱和批量处理功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def load_array(data_arrays, batch_size, is_train=True, num_workers=4): \n",
    "    \"\"\"构造一个 PyTorch 数据迭代器，支持多线程数据加载\"\"\"\n",
    "    dataset = TensorDataset(*data_arrays)  # 打包数据\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=is_train, num_workers=num_workers)  # 返回可迭代对象\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 区别二：定义模型结构和参数初始化\n",
    "- **非基于框架**：\n",
    "手动初始化模型参数，并手动实现前向传播。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 手动参数初始化\n",
    "w = torch.normal(mean=0, std=0.01, size=(2, 1), requires_grad=True)  # 随机初始化权重\n",
    "b = torch.zeros(1, requires_grad=True)  # 初始化偏差为 0\n",
    "\n",
    "# 自定义线性回归模型\n",
    "def linreg(X, w, b):\n",
    "    return torch.matmul(X, w) + b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **基于框架**：\n",
    "使用 nn.Sequential 定义模型结构，并初始化参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# 定义模型结构：线性回归\n",
    "net = nn.Sequential(nn.Linear(2, 1))\n",
    "\n",
    "# 初始化模型参数\n",
    "net[0].weight.data.normal_(0, 0.01)  # 随机初始化权重\n",
    "net[0].bias.data.fill_(0)  # 初始化偏差为 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 区别三：定义损失函数\n",
    "- **非基于框架**：\n",
    "手动定义平方损失函数，并确保数值稳定性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_loss(y_hat, y):\n",
    "    \"\"\"平方损失函数\"\"\"\n",
    "    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **基于框架**：\n",
    "直接使用 PyTorch 提供的 nn.MSELoss()，这是标准化的实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 区别四：定义优化方法\n",
    "- **非基于框架**：\n",
    "手动实现随机梯度下降（SGD），并确保梯度在每次更新后清零。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, lr, batch_size):\n",
    "    with torch.no_grad():  # 禁用梯度跟踪以节省内存\n",
    "        for param in params:\n",
    "            param -= lr * param.grad / batch_size  # 更新参数\n",
    "            param.grad.zero_()  # 将梯度清零\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **基于框架**：\n",
    "直接调用库中提供的优化器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = torch.optim.SGD(net.parameters(), lr=0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 区别五：模型训练\n",
    "模型训练包括**注入数据，正向传播，计算损失，反向传播，计算梯度，更新参数**。<br>\n",
    "- **非基于框架**：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.03\n",
    "num_epochs = 2\n",
    "batch_size = 16\n",
    "features = ''\n",
    "labels = ''\n",
    "for epoch in range(num_epochs):\n",
    "    for X , y in data_iter(batch_size, features,labels):\n",
    "        l = loss(net(X,w,b),y)\n",
    "        l.sum().backward() \n",
    "        # l.backward() PyTorch的backward()方法要求计算的目标是标量，而不是张量\n",
    "        sgd([w,b],lr,batch_size)\n",
    "    with torch.no_grad():\n",
    "        print(net(features,w,b).shape)\n",
    "        train_l = loss(net(features,w,b),labels)\n",
    "        print(f'epoch: {epoch + 1}, loss : {float(train_l.mean()):f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **基于框架**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr在定义优化方法的时候已经给定了\n",
    "num_epoch = 3\n",
    "for epoch in range(num_epoch):\n",
    "    for X,y in data_iter:\n",
    "        l = loss(net(X), y) #不需要输入参数\n",
    "        trainer.zero_grad() #将梯度置零 A\n",
    "        l.backward() #不需要sum(),已内置\n",
    "        trainer.step()\n",
    "\t\t\t\t#用于更新参数 B\n",
    "\t\t\t\t#A,B这两步在非基于框架中在SGD函数中体现\n",
    "    total_l =  loss(net(features),labels) \n",
    "    print(f\"epoch{epoch+1}, loss{total_l:f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
