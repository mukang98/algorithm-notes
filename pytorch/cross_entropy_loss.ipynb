{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 交叉熵\n",
    "在信息论中，交叉熵衡量的是当你有一个估计的概率分布 q，并用这个分布来编码信息时，\n",
    "平均需要多少位（比特）来正确地识别一个事件。简而言之，交叉熵描述了使用非最佳概率分布进行编码时，传达事件所需的额外信息量。交叉熵函数和负对数似然形式等价。\n",
    "由于y是一个 长度为q的独热编码向量，所以除了一个项以外的所有项j都消失了。由于所有$y_j$都是预测的概率，所以它们的对数永远不会大于0 <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "补充，熵的公式、相对熵（KL散度）公式，以及它们与交叉熵的关系。<br>\n",
    "熵的公式(熵就是平均信息量,信息量的公式为$I(x) = - \\log p(x)$)：<br>\n",
    "$H(p) = - \\sum_{i=1}^{N} p(x_i) \\log p(x_i)$ <br>\n",
    "相对熵（KL散度）公式：<br>\n",
    "$D_{KL}(P\\| Q) = \\sum_{i=1}^{N} p(x_i) \\log\\left(\\frac{p(x_i)}{q(x_i)}\\right)$<br>\n",
    "熵、交叉熵和KL散度的关系——交叉熵可以分解为熵和KL散度的和：<br>\n",
    "$H(P,Q) = H(P)+D_{KL}(P\\| Q)$<br>\n",
    "交叉熵的公式：<br>\n",
    "$H(P, Q) = - \\sum_{i=1}^{N} p(x_i) \\log q(x_i)$ <br>\n",
    "其中：<br>\n",
    "- $p(x_i)$是真实分布P中事件$x_i$发生的概率<br>\n",
    "- $q(x_i)$是我们用近似分布Q对事件$x_i$的预测概率。<br>\n",
    "\n",
    "这个公式表示，在真实分布下下事件发生的频率为$p(x_i)$时，我们用分布Q来编码这些事件所需的平均信息量。直观地说，如果我们一直用Q来编码或预测P中的事件，我们每次会“付出”多少信息代价(“代价”指的是我们需要消耗多少比特或多少单位的信息来正确地编码或预测某个事件。)。<br>\n",
    "简化为分类问题中使用独热编码的形式<br>\n",
    "$H(P, Q) = - \\sum_{i=1}^{N} y_i \\log \\hat{y}_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 调用CrossEntropyLoss标准库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.],\n",
      "        [20., 21., 22., 23.],\n",
      "        [24., 25., 26., 27.],\n",
      "        [28., 29., 30., 31.]]) tensor([1, 2, 3, 3, 2, 3, 1, 0])\n",
      "tensor([[[ 0.,  1.,  2.,  3.],\n",
      "         [ 4.,  5.,  6.,  7.],\n",
      "         [ 8.,  9., 10., 11.],\n",
      "         [12., 13., 14., 15.]],\n",
      "\n",
      "        [[16., 17., 18., 19.],\n",
      "         [20., 21., 22., 23.],\n",
      "         [24., 25., 26., 27.],\n",
      "         [28., 29., 30., 31.]]])\n",
      "tensor([[1, 2, 3, 3],\n",
      "        [2, 3, 1, 0]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "loss = nn.CrossEntropyLoss(weight=torch.tensor([1,2,3,5]).float(),ignore_index=-100,reduction=\"mean\")\n",
    "_inputs = torch.arange(32).float().reshape(2, 4, 4) #2个批次，每个批次 4个样本，然后维度（输出类别个数）为4\n",
    "_targets = torch.tensor([[1, 2, 3, 3], [2, 3, 1, 0]], dtype=torch.long) # 2个批次，每个批次4个样本对应输出的target的index\n",
    "output = loss(_inputs.view(-1,4), _targets.view(-1))\n",
    "print(_inputs.view(-1,4), _targets.view(-1))\n",
    "print(_inputs)\n",
    "print(_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 手动实现交叉熵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 3, 2, 3, 1, 0])\n",
      "media:tensor([1, 2, 3, 4, 5, 7])\n",
      "targettensor([2, 3, 3, 2, 3, 0])\n",
      "torch.Size([6, 4])\n",
      "tensor([[0, 0, 1, 0],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 1, 0],\n",
      "        [0, 0, 0, 1],\n",
      "        [1, 0, 0, 0]])\n",
      "tensor([2, 3, 3, 2, 3, 0])\n",
      "!!!\n",
      "tensor([1., 2., 3., 5.])\n",
      "tensor([3., 5., 5., 3., 5., 1.])\n"
     ]
    }
   ],
   "source": [
    "class CrossEntropy(nn.Module):\n",
    "    def __init__(self, weights = None, reduction = \"mean\", ignore_index = -100) -> None:\n",
    "        super().__init__()\n",
    "        self.weights = weights\n",
    "        self.reduction = reduction\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        #ignore_index\n",
    "        print(target)\n",
    "        index_target = torch.where(target != self.ignore_index)[0] #返回target中值不为ignore_index的索引\n",
    "\n",
    "        print(f\"media:{index_target}\")\n",
    "        # target = torch.gather(target, 0, index_target)\n",
    "        target = target[index_target]\n",
    "\n",
    "        print(f\"target{target}\")\n",
    "        input = input[index_target]\n",
    "        print(input.shape)\n",
    "\n",
    "        print(F.one_hot(target,input.shape[-1])) #input.shape[-1]为label的长度\n",
    "        #计算每一个样本的loss\n",
    "        f_loss = -torch.mul(F.one_hot(target,input.shape[-1]), torch.log_softmax(input, dim=1)) \n",
    "        # f_loss = -F.log_softmax(input, dim=1)[target] 之后看看\n",
    "        #相同的作用\n",
    "        # inputs_log_softmax = torch.log_softmax(inputs, dim=-1)\n",
    "        # log_softmax_prob = torch.gather(inputs_log_softmax, dim=-1, index=targets.view(-1, 1))\n",
    "        each_batch_loss = torch.sum(f_loss, dim=-1)\n",
    "        # print(\"###\")\n",
    "        # print(each_batch_loss)\n",
    "        # print(each_batch_loss.shape)\n",
    "\n",
    "        #Weights\n",
    "        if self.weights is not None:\n",
    "            # print(target)\n",
    "            # print(\"!!!\")\n",
    "            # print(self.weights)\n",
    "            self.weights = self.weights[target]\n",
    "            # print(self.weights)\n",
    "            each_batch_loss = torch.mul(self.weights, each_batch_loss)\n",
    "            if self.reduction == \"mean\":\n",
    "                # print(self.weights)\n",
    "                # print(torch.sum(self.weights) )\n",
    "                loss =  torch.sum(each_batch_loss)/torch.sum(self.weights) \n",
    "            elif self.reduction == \"sum\":\n",
    "                loss =  torch.sum(each_batch_loss)\n",
    "            else:\n",
    "                return each_batch_loss\n",
    "\n",
    "        #计算总loss\n",
    "        else:\n",
    "            if self.reduction == \"mean\":\n",
    "                loss = torch.mean(each_batch_loss)\n",
    "            elif self.reduction == \"sum\":\n",
    "                loss = torch.sum(each_batch_loss)\n",
    "            else:\n",
    "                return each_batch_loss\n",
    "        \n",
    "        return loss\n",
    "\n",
    "loss2 = CrossEntropy(weights=torch.tensor([1,2,3,5]).float(),ignore_index=1,reduction=\"mean\")\n",
    "output2 = loss2(_inputs.view(-1,4), _targets.view(-1))\n",
    "# print(output2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
